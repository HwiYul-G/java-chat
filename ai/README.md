# Artificial Intelligence, AI
##### ✏️ 목표
채팅 서비스에 제공할 비속어 탐지 모델 만들기</br>
##### 📑 목차
2. [사용 데이터](#사용-데이터)  
3. 학습 코드  
4. 학습 결과
5. 

## 사용 데이터
🔗 **[사용 데이터 링크](https://github.com/2runo/Curse-detection-data)**  
5828개의 댓글 데이터로, 각 댓글의 내용과 비속어 포함 여부가 `|` 구분자로 나뉘어 있습니다.

### 데이터 형식
- **총 수량**: 5,828개
- **구분 기준**: 
  - `0`: 비속어 없음
  - `1`: 비속어 있음
- **비속어 판단 기준**
  - 단순히 비속어 포함 유무가 아니라, 댓글의 전체 **맥락**을 통해 비속어로 판단한다.

### 예시
```text
집에 롱패딩만 세 개다. 10년 더 입어야지 | 0
애새끼가 초딩도 아니고 ㅋㅋㅋㅋ	| 1
```

## 
트랜스포머는 2017년 구글이 발표한 논문인 'Attention is all you need'에서 나온 모델로
기존의 seq2seq의 구조인 인코더-디코더를 따르면서도 attention만으로 구현한 모델이다.
-> attention만으로 인코더-디코더 구조를 따름
rNN을 사용하지 않고 인코더-디코더 구조로 설계했음에도 번역 성능에서 RNN보다 우수한 성능을 보여주었다.

## 필수?
1. [CLS]로 시작하고 [SEP]로 끝나게 만든다.
2. [CLS],[SEP]를 붙인 문장을 토큰화 한다.
  - # 이 붙는 글씨로 변경된다.
3. 문장의 최대 시퀀스를 128로 설정한다
  - 토큰화된 문장을 ids로 전환한다.
  - 전환된 ids에 제로 패딩을 수행한다. (128 외의 곳에는 0을 넣음)
4. attention mask
  - 제로패딩을 넣은 ids에 어텐션 마스크를 만든다.
  - 0값을 가지는 곳은 0을 넣고 그 외는 1을 넣는다.
  - 이 어텐션 마스크를 통해서 패딩 토큰에 대해 불필요한 어텐션 연산을 수행하지 않게 한다.
5. bert model을 불러온다.
  - label은 0 혹은 1 이므로 2로 설정
  - 옵티마이저는 adamW, 학습률은 2e-5, eps=1e-8
  - 에폭: 4
  - 총 훈련 스텝 = 배치 반복 횟수 * 에폭
  - 스케줄러 작성
  - 학습
    - 

## 참고자료
- [일상/연예 주제의 한국어 대화 'BERT'로 이진 분류 모델 만들기](https://velog.io/@seolini43/%EC%9D%BC%EC%83%81%EC%97%B0%EC%95%A0-%EC%A3%BC%EC%A0%9C%EC%9D%98-%ED%95%9C%EA%B5%AD%EC%96%B4-%EB%8C%80%ED%99%94-BERT%EB%A1%9C-%EC%9D%B4%EC%A7%84-%EB%B6%84%EB%A5%98-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0%ED%8C%8C%EC%9D%B4%EC%8D%ACColab-%EC%BD%94%EB%93%9C#2train-set--test-set%EC%9C%BC%EB%A1%9C-%EB%82%98%EB%88%84%EA%B8%B0)