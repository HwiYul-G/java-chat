# Artificial Intelligence, AI
##### âœï¸ ëª©í‘œ
ì±„íŒ… ì„œë¹„ìŠ¤ì— ì œê³µí•  ë¹„ì†ì–´ íƒì§€ ëª¨ë¸ ë§Œë“¤ê¸°</br>
##### ğŸ“‘ ëª©ì°¨
1. [ì‚¬ìš© ë°ì´í„°](#ì‚¬ìš©-ë°ì´í„°)  
   1. [ë°ì´í„° í˜•ì‹](#ë°ì´í„°-í˜•ì‹)
   2. [ë°ì´í„° ì˜ˆì‹œ](#ë°ì´í„°-ì˜ˆì‹œ)
2. [**ì½”ë“œ ì „ í•„ìˆ˜ ì§€ì‹**](#ì½”ë“œ-ì „-í•„ìˆ˜-ì§€ì‹)
3. [í•™ìŠµ ë° í‰ê°€ ì½”ë“œ](#í•™ìŠµ-ë°-í‰ê°€-ì½”ë“œ)
   1. [ë°ì´í„° ì „ì²˜ë¦¬](#ë°ì´í„°-ì „ì²˜ë¦¬)
      1. [ì‹œì‘ê³¼ ë í† í° ì¶”ê°€](#ì‹œì‘ê³¼-ë-í† í°-ì¶”ê°€)
      2. [í† í°í™”](#í† í°í™”)
      3. [ì •ìˆ˜ ì¸ì½”ë”©ê³¼ ì œë¡œ íŒ¨ë”©](#ì •ìˆ˜-ì¸ì½”ë”©ê³¼-ì œë¡œ-íŒ¨ë”©)
      4. [ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±](#ì–´í…ì…˜-ë§ˆìŠ¤í¬-ìƒì„±)
   2. [ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ](#ëª¨ë¸-ì„¤ì •-ë°-í•™ìŠµ)
      1. [Bert ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°](#bert-ëª¨ë¸-ë¶ˆëŸ¬ì˜¤ê¸°)
      2. [í•™ìŠµ ì„¤ì • ì…‹íŒ…](#í•™ìŠµ-ì„¤ì •-ì…‹íŒ…)
      3. [í•™ìŠµ ê³¼ì •](#í•™ìŠµ-ê³¼ì •)
   3. [í‰ê°€ ë° ê²°ê³¼](#í‰ê°€-ë°-ê²°ê³¼)
      1. [í‰ê°€ ë° ê²°ê³¼](#í‰ê°€-ë°-ê²°ê³¼)
      2. [ìƒˆë¡œìš´ ë¬¸ì¥ í…ŒìŠ¤íŠ¸](#ìƒˆë¡œìš´-ë¬¸ì¥-í…ŒìŠ¤íŠ¸)

## ì‚¬ìš© ë°ì´í„°
ğŸ”— **[ì‚¬ìš© ë°ì´í„° ë§í¬](https://github.com/2runo/Curse-detection-data)**  
5828ê°œì˜ ëŒ“ê¸€ ë°ì´í„°ë¡œ, ê° ëŒ“ê¸€ì˜ ë‚´ìš©ê³¼ ë¹„ì†ì–´ í¬í•¨ ì—¬ë¶€ê°€ `|` êµ¬ë¶„ìë¡œ ë‚˜ë‰˜ì–´ ìˆìŠµë‹ˆë‹¤.

### ë°ì´í„° í˜•ì‹
- **ì´ ìˆ˜ëŸ‰**: 5,828ê°œ
- **êµ¬ë¶„ ê¸°ì¤€**: 
  - `0`: ë¹„ì†ì–´ ì—†ìŒ
  - `1`: ë¹„ì†ì–´ ìˆìŒ
- **ë¹„ì†ì–´ íŒë‹¨ ê¸°ì¤€**
  - ë‹¨ìˆœíˆ ë¹„ì†ì–´ í¬í•¨ ìœ ë¬´ê°€ ì•„ë‹ˆë¼, ëŒ“ê¸€ì˜ ì „ì²´ **ë§¥ë½**ì„ í†µí•´ ë¹„ì†ì–´ë¡œ íŒë‹¨í•œë‹¤.

### ë°ì´í„° ì˜ˆì‹œ
```text
ì§‘ì— ë¡±íŒ¨ë”©ë§Œ ì„¸ ê°œë‹¤. 10ë…„ ë” ì…ì–´ì•¼ì§€ | 0
ì• ìƒˆë¼ê°€ ì´ˆë”©ë„ ì•„ë‹ˆê³  ã…‹ã…‹ã…‹ã…‹	| 1
```

## ì½”ë“œ ì „ í•„ìˆ˜ ì§€ì‹
#### ìƒ˜í”Œ ë¬¸ì¥
```text
ë¬¸ì¥1: íŒŒë¦¬ëŠ” ì•„ë¦„ë‹¤ìš´ ë„ì‹œì´ë‹¤.
ë¬¸ì¥2: ë‚˜ëŠ” íŒŒë¦¬ë¥¼ ì‚¬ë‘í•´ìš”.
```
### ì…ë ¥ ë°ì´í„° í‘œí˜„
- í† í° ì„ë² ë”©, ì„¸ê·¸ë¨¼íŠ¸ ì„ë©”ë”©, í¬ì§€ì…˜(ìœ„ì¹˜) ì„ë² ë”©

#### í† í° ì„ë² ë”©
- `[CLS]`ë¥¼ ì‹œì‘ì— ë¶™ì´ê³  `[SEP]`ë¥¼ ëì— ë¶™ì¸ë‹¤.
  - `[CLS]`ëŠ” ë¶„ë¥˜ ì‘ì—…ì„ ìœ„í•´ ì‚¬ìš©í•œë‹¤.
  - `[SEP]`ëŠ” ë¬¸ì¥ì˜ ëì— ë“±ì¥í•œë‹¤.
- ë¬¸ì¥1: [[cls], íŒŒë¦¬ëŠ”, ì•„ë¦„ë‹¤ìš´, ë„ì‹œì´ë‹¤., [sep], ë‚˜ëŠ”, íŒŒë¦¬ë¥¼, ì‚¬ë‘í•´ìš”.]

#### ì„¸ê·¸ë¨¼íŠ¸ ì„ë² ë”©
ë¬¸ì¥1ì— ì†í•˜ëŠ”ì§€ ë¬¸ì¥2ì— ì†í•˜ëŠ”ì§€ í‘œê¸°í•œë‹¤.
```text
[CLS] : ë¬¸ì¥1
íŒŒë¦¬ëŠ”: ë¬¸ì¥1
ì•„ë¦„ë‹¤ìš´: ë¬¸ì¥1
ë„ì‹œì´ë‹¤.: ë¬¸ì¥1
[SEP]: ë¬¸ì¥1
ë‚˜ëŠ”: ë¬¸ì¥2
íŒŒë¦¬ë¥¼: ë¬¸ì¥2
ì‚¬ë‘í•´ìš”.: ë¬¸ì¥2
```

#### í¬ì§€ì…˜ ì„ë² ë”©
- íŠ¸ë ŒìŠ¤í¬ë¨¸ëŠ” recurrence ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëª¨ë“  ë‹¨ì–´ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì—, ë‹¨ì–´ ìˆœì„œì™€ ê´€ë ¨ëœ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤.
- ê·¸ë˜ì„œ positional encodingì´ í•„ìš”í•˜ë‹¤.

### WordPiece Tokenizer
- ì›Œë“œí”¼ìŠ¤ í† í¬ë‚˜ì´ì €ëŠ” ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì € ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥¸ë‹¤. 
- `Let us start pretraining the model`ë¥¼ ì›Œë“œ í”¼ìŠ¤ë¥¼ ì‚¬ìš©í•´
- í† í¬ë‚˜ì´ì¦ˆí•˜ë©´ `[let, us, start, pre, ###train, ###ing, the, model]`ì´ ëœë‹¤.
- vocabulary wordsì—ì„œ ë‹¤ë£° ìˆ˜ ìˆì„ ì •ë„ë¡œ wordë¥¼ ì¬ê·€ì ìœ¼ë¡œ ìª¼ê° ë‹¤.

## í•™ìŠµ ë° í‰ê°€ ì½”ë“œ
### ë°ì´í„° ì „ì²˜ë¦¬
#### ì‹œì‘ê³¼ ë í† í° ì¶”ê°€
```python
sentences = ['[CLS]' + str(s) + '[SEP]' for s in train['content]]
```
#### í† í°í™”
```python
# ë‹¨ì–´ë¥¼ í† í°í™”í•œë‹¤. `##` ê°™ì€ ê²ƒì´ ìª¼ê°œì„œ ë¶™ëŠ”ë‹¤.
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)
tokenized_texts = [tokenizer.tokenize(s) for s in sentences]
```
#### ì˜ˆì‹œ
```text
[CLS] ì„ìœ  í™”í•™ì€ GS, S-ì˜¤ì¼, í˜„ëŒ€ì˜¤ì¼ë±…í¬ì§€ [SEP]
['[CLS]', 'ì„', '##ìœ ', 'í™”', '##í•™', '##ì€', 'GS', ',', 'S', '-', 'ì˜¤', '##ì¼', ',', 'í˜„ëŒ€', '##ì˜¤', '##ì¼', '##ë±…', '##í¬', '##ì§€', '[SEP]']
```
#### ì •ìˆ˜ ì¸ì½”ë”©ê³¼ ì œë¡œ íŒ¨ë”©
```python
MAX_LEN = 128

input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]
input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
```
#### ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
```python
attention_masks = []

# input_idsì—ëŠ” ë¬¸ì¥ì„ 128ê°œì˜ ì‹œí€€ìŠ¤ë¡œ ë‚˜ëˆˆ ë°ì´í„°ê°€ ë“¤ì–´ìˆìŒ
for seq in input_ids: 
  seq_mask = [ float(i > 0) for i in seq ]
  attention_masks.append(seq_mask)

# attention maskì—ëŠ” ë°ì´í„°ì˜ ìœ ë¬´(1 or 0)ë§Œ ë“¤ì–´ê°€ ìˆìŒ
```

### ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ
#### Bert ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
```python
model = BertForSequenceClassification.from_pretrained("bert-base-multilingual-cased", num_labels=2)
model.cuda()
```
#### í•™ìŠµ ì„¤ì • ì…‹íŒ…
```python
# ì˜µí‹°ë§ˆì´ì €
optimizer = AdamW(
  model.parameters(),
  lr = 2e-5, # í•™ìŠµë¥ (learning rate)
  eps = 1e-8
)

# ì—í­ìˆ˜
epochs = 4

# ì´ í›ˆë ¨ ìŠ¤í…: ë°°ì¹˜ë°˜ë³µ íšŸìˆ˜ * ì—í­
total_steps = len(train_dataloader) * epochs

# ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps = 0,
  num_training_steps = total_steps
)
```
#### í•™ìŠµ ê³¼ì •
```python
# ëœë¤ ì‹œë“œ ê³ ì •
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

# ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
model.zero_grad()

# í•™ìŠµ
for epoch_i in range(0, epochs):
  ...
  model.train() # í›ˆë ¨ ëª¨ë“œë¡œ ë³€ê²½
  
  for step, batch in enumerate(train_dataloader):
    ...
    # forward ìˆ˜í–‰
    outputs = model(b_input_ids, token_type_ids=None, attention_mask= b_input_mask, labels=b_labels)
    # loss êµ¬í•¨
    loss = outputs[0]
    ...
    # backward ìˆ˜í–‰ > gradient ê³„ì‚°
    loss.backward()
    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    # ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
    optimizer.step()
    # ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ í•™ìŠµë¥  ê°ì†Œ
    scheduler.step()
    ...
    
    # === Validation ===
```

### í‰ê°€ ë° ê²°ê³¼
```python
...
model.eval() # í‰ê°€ ëª¨ë“œë¡œ ë³€ê²½
...
for step, batch in enumerate(test_dataloader):
  ...
  # gradient ê³„ì‚° ì•ˆí•˜ê³  forwardë§Œ ìˆ˜í–‰
  with torch.no_grad():
    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
  
  ...
```
```text
Accuracy: 0.83
Test took: 0:00:04
```

#### ìƒˆë¡œìš´ ë¬¸ì¥ í…ŒìŠ¤íŠ¸
```python
def convert_input_data(sentences):
  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]
  MAX_LEN = 128
  ...
  
  inputs = torch.tensor(input_ids) # dataë¥¼ ê°€ì§„ token-idsë¥¼ tensorë¡œ
  masks = torch.tensor(attention_masks) # attention mask to tensor
  
  return inputs, masks 
```
```python
def test_sentences(sentences):
  model.eval() # í‰ê°€ëª¨ë“œë¡œ ë³€ê²½
  inputs, masks = convert_input_data(sentences)
  ...
  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° X
  with torch.no_grad():
    # forward ìˆ˜í–‰
    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)
  
  logits = outputs[0] # ë¡œìŠ¤ êµ¬í•¨
  logits = logits.detach().cpu().numpy() # cpuë¡œ ë°ì´í„° ì´ë™
  return logits
```
```python
logits = test_sentences(['ã……ã…‚ë†ˆì˜ ìƒˆë¼ë“¤ì´ ë‹´ë°° ì¡´ë‚˜ í”¼ë„¤ ì‹œë°œ ì§€ ë°©ì•ˆì—ì„œ ë‚˜ í”¼ì§€:)'])
print(logits)

if np.argmax(logits) == 1 :
    print("ë¹„ì†ì–´ ì…ë‹ˆë‹¤.")
elif np.argmax(logits) == 0 :
    print("ë¹„ì†ì–´ê°€ ì•„ë‹™ë‹ˆë‹¤.")
```
```text
[[-2.2733462  2.191932 ]]
ë¹„ì†ì–´ ì…ë‹ˆë‹¤.
```

## ì°¸ê³ ìë£Œ
- https://medium.com/analytics-vidhya/understanding-the-bert-model-a04e1c7933a9
- https://velog.io/@seolini43/%EC%9D%BC%EC%83%81%EC%97%B0%EC%95%A0-%EC%A3%BC%EC%A0%9C%EC%9D%98-%ED%95%9C%EA%B5%AD%EC%96%B4-%EB%8C%80%ED%99%94-BERT%EB%A1%9C-%EC%9D%B4%EC%A7%84-%EB%B6%84%EB%A5%98-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0%ED%8C%8C%EC%9D%B4%EC%8D%ACColab-%EC%BD%94%EB%93%9C#2train-set--test-set%EC%9C%BC%EB%A1%9C-%EB%82%98%EB%88%84%EA%B8%B0